{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3jnTIaA7nnk",
        "outputId": "b0cb342b-45a9-4c0c-a1db-9541251bf816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done.\n",
            "ROOT: /content\n",
            "Data saved: /content/data/synthetic_fraud_transactions.csv\n",
            "Scored output saved: /content/outputs/scored_test_transactions.csv\n",
            "ROC-AUC: 0.7705 | PR-AUC: 0.5974\n",
            "\n",
            "Decisioning evaluation (DECLINE as positive prediction):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7713    0.9137    0.8365     11139\n",
            "           1     0.6573    0.3791    0.4809      4861\n",
            "\n",
            "    accuracy                         0.7513     16000\n",
            "   macro avg     0.7143    0.6464    0.6587     16000\n",
            "weighted avg     0.7367    0.7513    0.7285     16000\n",
            "\n",
            "Saved outputs in: /content/outputs\n"
          ]
        }
      ],
      "source": [
        "# Fraud Decisioning & Risk Scoring Framework (Credit Card / Customer Fraud)\n",
        "# ✅ Colab / Jupyter-friendly (no __file__ dependency)\n",
        "# ✅ Generates synthetic transaction data (safe), trains model, scores, decisioning, saves outputs + charts\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, classification_report,\n",
        "    confusion_matrix, precision_recall_curve, roc_curve\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Paths (Notebook-safe)\n",
        "# ----------------------------\n",
        "ROOT = os.getcwd()\n",
        "DATA_DIR = os.path.join(ROOT, \"data\")\n",
        "OUT_DIR = os.path.join(ROOT, \"outputs\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Synthetic Data Generator\n",
        "# ----------------------------\n",
        "def generate_synthetic_fraud_data(\n",
        "    n_customers: int = 6000,\n",
        "    n_transactions: int = 80000,\n",
        "    start_date: str = \"2025-01-01\",\n",
        "    days: int = 120\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a realistic-ish credit card transaction dataset with:\n",
        "    - customer profile (tenure, prior chargebacks)\n",
        "    - transaction attributes (amount, MCC, country)\n",
        "    - risk signals (device change, velocity_1h/24h, failed_logins_24h, travel_flag)\n",
        "    - label: is_fraud (imbalanced)\n",
        "    \"\"\"\n",
        "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "\n",
        "    # Customer-level attributes\n",
        "    customer_ids = np.arange(1, n_customers + 1)\n",
        "\n",
        "    account_age_days = np.random.gamma(shape=2.0, scale=250.0, size=n_customers).astype(int) + 10\n",
        "    account_age_days = np.clip(account_age_days, 10, 3650)\n",
        "\n",
        "    # \"prior_chargebacks\" as a rare-ish history signal\n",
        "    prior_chargebacks = np.random.poisson(lam=0.12, size=n_customers)\n",
        "    prior_chargebacks = np.clip(prior_chargebacks, 0, 6)\n",
        "\n",
        "    # Baseline risk propensity per customer (latent)\n",
        "    cust_risk = np.random.beta(a=2.0, b=15.0, size=n_customers)  # most low-risk\n",
        "\n",
        "    customers = pd.DataFrame({\n",
        "        \"customer_id\": customer_ids,\n",
        "        \"account_age_days\": account_age_days,\n",
        "        \"prior_chargebacks\": prior_chargebacks,\n",
        "        \"cust_risk_propensity\": cust_risk\n",
        "    })\n",
        "\n",
        "    # Transaction-level generation\n",
        "    # Pick customers for each transaction\n",
        "    tx_customer = np.random.choice(customer_ids, size=n_transactions, replace=True)\n",
        "\n",
        "    # Transaction time spread\n",
        "    tx_time_offsets = np.random.randint(0, days * 24 * 60, size=n_transactions)\n",
        "    tx_time = [start_dt + timedelta(minutes=int(m)) for m in tx_time_offsets]\n",
        "\n",
        "    # Amount distribution (log-normal-ish)\n",
        "    amount = np.random.lognormal(mean=3.5, sigma=0.9, size=n_transactions)  # around tens/hundreds\n",
        "    amount = np.clip(amount, 1, 2500).round(2)\n",
        "\n",
        "    # Merchant categories (MCC-like)\n",
        "    mcc = np.random.choice(\n",
        "        [\"grocery\", \"gas\", \"electronics\", \"online_retail\", \"travel\", \"restaurants\", \"pharmacy\", \"luxury\"],\n",
        "        size=n_transactions,\n",
        "        p=[0.18, 0.12, 0.10, 0.22, 0.08, 0.16, 0.08, 0.06]\n",
        "    )\n",
        "\n",
        "    # Geo/country\n",
        "    country = np.random.choice(\n",
        "        [\"US\", \"CA\", \"MX\", \"GB\", \"IN\", \"AE\", \"NG\", \"BR\", \"FR\", \"DE\"],\n",
        "        size=n_transactions,\n",
        "        p=[0.72, 0.06, 0.05, 0.03, 0.03, 0.02, 0.02, 0.03, 0.02, 0.02]\n",
        "    )\n",
        "\n",
        "    # \"card_present\" signal\n",
        "    card_present = np.where(\n",
        "        np.isin(mcc, [\"grocery\", \"gas\", \"restaurants\", \"pharmacy\"]),\n",
        "        np.random.binomial(1, 0.78, size=n_transactions),\n",
        "        np.random.binomial(1, 0.22, size=n_transactions)\n",
        "    )\n",
        "\n",
        "    # Device change (new device / new browser) more common on online\n",
        "    device_change = np.where(\n",
        "        np.isin(mcc, [\"online_retail\", \"electronics\", \"travel\"]),\n",
        "        np.random.binomial(1, 0.16, size=n_transactions),\n",
        "        np.random.binomial(1, 0.06, size=n_transactions)\n",
        "    )\n",
        "\n",
        "    # Failed login attempts in last 24h (ATO signal) — mostly zero\n",
        "    failed_logins_24h = np.random.poisson(lam=0.25, size=n_transactions)\n",
        "    failed_logins_24h = np.clip(failed_logins_24h, 0, 8)\n",
        "\n",
        "    # Travel flag (geo mismatch proxy) — rare\n",
        "    travel_flag = np.random.binomial(1, 0.04, size=n_transactions)\n",
        "\n",
        "    # Velocity features (transactions counts) — base + uplift from online and ATO-like patterns\n",
        "    base_v1h = np.random.poisson(lam=0.18, size=n_transactions)  # typically 0/1\n",
        "    base_v24 = np.random.poisson(lam=1.6, size=n_transactions)   # typically 0-5\n",
        "    uplift_online = np.where(np.isin(mcc, [\"online_retail\", \"electronics\"]), 1, 0)\n",
        "    uplift_ato = np.where(failed_logins_24h >= 2, 1, 0)\n",
        "\n",
        "    velocity_1h = base_v1h + np.random.binomial(1, 0.12, size=n_transactions) + uplift_online + uplift_ato\n",
        "    velocity_24h = base_v24 + np.random.poisson(lam=0.6, size=n_transactions) + (2 * uplift_online) + (2 * uplift_ato)\n",
        "\n",
        "    velocity_1h = np.clip(velocity_1h, 0, 15)\n",
        "    velocity_24h = np.clip(velocity_24h, 0, 60)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"transaction_id\": np.arange(1, n_transactions + 1),\n",
        "        \"customer_id\": tx_customer,\n",
        "        \"transaction_time\": pd.to_datetime(tx_time),\n",
        "        \"transaction_amount\": amount,\n",
        "        \"merchant_category\": mcc,\n",
        "        \"transaction_country\": country,\n",
        "        \"card_present\": card_present.astype(int),\n",
        "        \"device_change\": device_change.astype(int),\n",
        "        \"failed_logins_24h\": failed_logins_24h.astype(int),\n",
        "        \"travel_flag\": travel_flag.astype(int),\n",
        "        \"velocity_1h\": velocity_1h.astype(int),\n",
        "        \"velocity_24h\": velocity_24h.astype(int),\n",
        "    })\n",
        "\n",
        "    # Merge customer features\n",
        "    df = df.merge(customers.drop(columns=[\"cust_risk_propensity\"]), on=\"customer_id\", how=\"left\")\n",
        "    df = df.merge(customers[[\"customer_id\", \"cust_risk_propensity\"]], on=\"customer_id\", how=\"left\")\n",
        "\n",
        "    # Create a fraud probability using a logistic-like structure:\n",
        "    # - Amount high\n",
        "    # - Online/electronics/travel more risky\n",
        "    # - Non-card-present more risky\n",
        "    # - Device change, travel flag, failed logins, velocity higher -> more risky\n",
        "    # - Younger account age & prior chargebacks -> more risky\n",
        "    # - Customer latent propensity\n",
        "    mcc_risk_map = {\n",
        "        \"grocery\": 0.2,\n",
        "        \"gas\": 0.35,\n",
        "        \"electronics\": 0.8,\n",
        "        \"online_retail\": 0.9,\n",
        "        \"travel\": 0.85,\n",
        "        \"restaurants\": 0.3,\n",
        "        \"pharmacy\": 0.25,\n",
        "        \"luxury\": 0.95\n",
        "    }\n",
        "    country_risk_map = {\n",
        "        \"US\": 0.25, \"CA\": 0.30, \"MX\": 0.45, \"GB\": 0.35, \"IN\": 0.50,\n",
        "        \"AE\": 0.55, \"NG\": 0.75, \"BR\": 0.60, \"FR\": 0.35, \"DE\": 0.33\n",
        "    }\n",
        "\n",
        "    mcc_risk = df[\"merchant_category\"].map(mcc_risk_map).astype(float)\n",
        "    ctry_risk = df[\"transaction_country\"].map(country_risk_map).astype(float)\n",
        "\n",
        "    amt = df[\"transaction_amount\"].astype(float)\n",
        "    log_amt = np.log1p(amt)\n",
        "\n",
        "    # Normalize account age so newer accounts get higher risk\n",
        "    acc_age = df[\"account_age_days\"].astype(float)\n",
        "    newness = 1.0 - np.clip(acc_age / 3650.0, 0, 1)  # 0 (old) -> 1 (new)\n",
        "\n",
        "    prior_cb = df[\"prior_chargebacks\"].astype(float)\n",
        "\n",
        "    # Score components\n",
        "    z = (\n",
        "        -6.2\n",
        "        + 0.55 * log_amt\n",
        "        + 1.1 * (1 - df[\"card_present\"].astype(float))\n",
        "        + 0.9 * df[\"device_change\"].astype(float)\n",
        "        + 1.25 * df[\"travel_flag\"].astype(float)\n",
        "        + 0.38 * df[\"failed_logins_24h\"].astype(float)\n",
        "        + 0.22 * df[\"velocity_1h\"].astype(float)\n",
        "        + 0.10 * df[\"velocity_24h\"].astype(float)\n",
        "        + 0.9 * mcc_risk\n",
        "        + 0.7 * ctry_risk\n",
        "        + 1.1 * newness\n",
        "        + 0.75 * prior_cb\n",
        "        + 2.2 * df[\"cust_risk_propensity\"].astype(float)\n",
        "    )\n",
        "\n",
        "    p = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Impose class imbalance + some label noise\n",
        "    # Target fraud rate ~ 1% to 3% depending on parameters\n",
        "    is_fraud = np.random.binomial(1, np.clip(p, 0, 1))\n",
        "\n",
        "    # Small random flips to mimic imperfect labels\n",
        "    flip_mask = np.random.binomial(1, 0.002, size=n_transactions).astype(bool)\n",
        "    is_fraud[flip_mask] = 1 - is_fraud[flip_mask]\n",
        "\n",
        "    df[\"is_fraud\"] = is_fraud.astype(int)\n",
        "\n",
        "    # Drop latent propensity from model features (keep for generator only)\n",
        "    df = df.drop(columns=[\"cust_risk_propensity\"])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Risk Score + Decisioning\n",
        "# ----------------------------\n",
        "def prob_to_risk_score(prob: np.ndarray, min_score=0, max_score=1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert probability to a 0–1000 score (higher = riskier).\n",
        "    Simple linear mapping is fine for a portfolio project.\n",
        "    \"\"\"\n",
        "    prob = np.clip(prob, 0, 1)\n",
        "    score = (prob * (max_score - min_score) + min_score)\n",
        "    return np.round(score).astype(int)\n",
        "\n",
        "def apply_decision(score: np.ndarray, approve_max=300, review_max=700) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Approve / Review / Decline based on risk score thresholds.\n",
        "    \"\"\"\n",
        "    decision = np.where(score <= approve_max, \"APPROVE\",\n",
        "                np.where(score <= review_max, \"REVIEW\", \"DECLINE\"))\n",
        "    return decision\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Build + Train Model\n",
        "# ----------------------------\n",
        "def train_fraud_model(df: pd.DataFrame):\n",
        "    target = \"is_fraud\"\n",
        "\n",
        "    # Sort by time (optional realism)\n",
        "    df = df.sort_values(\"transaction_time\").reset_index(drop=True)\n",
        "\n",
        "    # Features\n",
        "    feature_cols = [\n",
        "        \"transaction_amount\",\n",
        "        \"merchant_category\",\n",
        "        \"transaction_country\",\n",
        "        \"card_present\",\n",
        "        \"device_change\",\n",
        "        \"failed_logins_24h\",\n",
        "        \"travel_flag\",\n",
        "        \"velocity_1h\",\n",
        "        \"velocity_24h\",\n",
        "        \"account_age_days\",\n",
        "        \"prior_chargebacks\"\n",
        "    ]\n",
        "\n",
        "    X = df[feature_cols].copy()\n",
        "    y = df[target].copy()\n",
        "\n",
        "    # Train/test split with stratification due to imbalance\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    numeric_features = [\n",
        "        \"transaction_amount\",\n",
        "        \"failed_logins_24h\",\n",
        "        \"velocity_1h\",\n",
        "        \"velocity_24h\",\n",
        "        \"account_age_days\",\n",
        "        \"prior_chargebacks\"\n",
        "    ]\n",
        "    categorical_features = [\"merchant_category\", \"transaction_country\"]\n",
        "    passthrough_features = [\"card_present\", \"device_change\", \"travel_flag\"]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", Pipeline(steps=[\n",
        "                (\"scaler\", StandardScaler())\n",
        "            ]), numeric_features),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
        "            (\"pass\", \"passthrough\", passthrough_features),\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "    # Logistic Regression baseline (strong + interpretable)\n",
        "    # class_weight balances minority fraud class\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=500,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=None\n",
        "    )\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predicted probabilities\n",
        "    p_test = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    roc = roc_auc_score(y_test, p_test)\n",
        "    ap = average_precision_score(y_test, p_test)\n",
        "\n",
        "    return model, (X_train, X_test, y_train, y_test, p_test, roc, ap)\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Threshold Calibration (Optional but nice)\n",
        "# ----------------------------\n",
        "def find_threshold_for_precision(y_true, y_prob, target_precision=0.80):\n",
        "    \"\"\"\n",
        "    Finds the smallest threshold that achieves >= target_precision.\n",
        "    Useful for \"DECLINE\" cutoff in fraud systems.\n",
        "    \"\"\"\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    # precision/recall arrays are len(thresholds)+1\n",
        "    # We'll iterate thresholds and pick first satisfying precision.\n",
        "    for i, t in enumerate(thresholds):\n",
        "        if precision[i] >= target_precision:\n",
        "            return float(t), float(precision[i]), float(recall[i])\n",
        "    # fallback highest threshold\n",
        "    return float(thresholds[-1]), float(precision[-2]), float(recall[-2])\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Plot Helpers\n",
        "# ----------------------------\n",
        "def save_confusion_matrix(cm, path):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(\"Confusion Matrix (Test)\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.xticks([0, 1], [\"Not Fraud\", \"Fraud\"])\n",
        "    plt.yticks([0, 1], [\"Not Fraud\", \"Fraud\"])\n",
        "    for (i, j), v in np.ndenumerate(cm):\n",
        "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def save_risk_score_distribution(df_scored, path):\n",
        "    plt.figure()\n",
        "    plt.hist(df_scored[\"risk_score\"], bins=50)\n",
        "    plt.title(\"Risk Score Distribution\")\n",
        "    plt.xlabel(\"Risk Score (0–1000)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def save_roc_curve(y_true, y_prob, path):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.title(\"ROC Curve (Test)\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def save_pr_curve(y_true, y_prob, path):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision)\n",
        "    plt.title(\"Precision-Recall Curve (Test)\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Main Run\n",
        "# ----------------------------\n",
        "def main():\n",
        "    # A) Create data\n",
        "    df = generate_synthetic_fraud_data(\n",
        "        n_customers=6000,\n",
        "        n_transactions=80000,\n",
        "        start_date=\"2025-01-01\",\n",
        "        days=120\n",
        "    )\n",
        "\n",
        "    # Save raw data\n",
        "    data_path = os.path.join(DATA_DIR, \"synthetic_fraud_transactions.csv\")\n",
        "    df.to_csv(data_path, index=False)\n",
        "\n",
        "    # B) Train model\n",
        "    model, pack = train_fraud_model(df)\n",
        "    X_train, X_test, y_train, y_test, p_test, roc, ap = pack\n",
        "\n",
        "    # C) Calibrate a \"decline\" threshold for a target precision (optional)\n",
        "    decline_thr, achieved_prec, achieved_rec = find_threshold_for_precision(\n",
        "        y_true=y_test.values,\n",
        "        y_prob=p_test,\n",
        "        target_precision=0.80\n",
        "    )\n",
        "\n",
        "    # We’ll map probs -> risk score; then set score thresholds to mimic:\n",
        "    # APPROVE <= 300, REVIEW 301-700, DECLINE > 700\n",
        "    # But also: if you want data-driven, you can set decline based on decline_thr\n",
        "    # Here: we keep fixed score thresholds (portfolio clarity) + report model-driven threshold.\n",
        "    risk_scores = prob_to_risk_score(p_test)  # 0–1000\n",
        "    decisions = apply_decision(risk_scores, approve_max=300, review_max=700)\n",
        "\n",
        "    # D) For evaluation, treat DECLINE as \"fraud predicted\" (binary)\n",
        "    y_pred_decline = (decisions == \"DECLINE\").astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_decline)\n",
        "    report = classification_report(y_test, y_pred_decline, digits=4)\n",
        "\n",
        "    # E) Build scored test output\n",
        "    df_test_scored = X_test.copy()\n",
        "    df_test_scored[\"actual_is_fraud\"] = y_test.values\n",
        "    df_test_scored[\"fraud_probability\"] = p_test\n",
        "    df_test_scored[\"risk_score\"] = risk_scores\n",
        "    df_test_scored[\"decision\"] = decisions\n",
        "\n",
        "    # Summary tables\n",
        "    decision_summary = (\n",
        "        df_test_scored\n",
        "        .groupby([\"decision\", \"actual_is_fraud\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        "        .sort_values([\"decision\", \"actual_is_fraud\"])\n",
        "    )\n",
        "\n",
        "    # \"Fraud rate by decision\" (great portfolio metric)\n",
        "    fraud_rate_by_decision = (\n",
        "        df_test_scored\n",
        "        .groupby(\"decision\")[\"actual_is_fraud\"]\n",
        "        .mean()\n",
        "        .reset_index(name=\"fraud_rate\")\n",
        "        .sort_values(\"fraud_rate\", ascending=False)\n",
        "    )\n",
        "\n",
        "    # Save outputs\n",
        "    scored_path = os.path.join(OUT_DIR, \"scored_test_transactions.csv\")\n",
        "    decision_summary_path = os.path.join(OUT_DIR, \"decision_summary.csv\")\n",
        "    fraud_rate_path = os.path.join(OUT_DIR, \"fraud_rate_by_decision.csv\")\n",
        "\n",
        "    df_test_scored.to_csv(scored_path, index=False)\n",
        "    decision_summary.to_csv(decision_summary_path, index=False)\n",
        "    fraud_rate_by_decision.to_csv(fraud_rate_path, index=False)\n",
        "\n",
        "    # Save charts\n",
        "    save_confusion_matrix(cm, os.path.join(OUT_DIR, \"confusion_matrix_decline_vs_actual.png\"))\n",
        "    save_risk_score_distribution(df_test_scored, os.path.join(OUT_DIR, \"risk_score_distribution.png\"))\n",
        "    save_roc_curve(y_test.values, p_test, os.path.join(OUT_DIR, \"roc_curve.png\"))\n",
        "    save_pr_curve(y_test.values, p_test, os.path.join(OUT_DIR, \"pr_curve.png\"))\n",
        "\n",
        "    # Executive summary markdown (portfolio-friendly)\n",
        "    exec_md = f\"\"\"# Fraud Decisioning & Risk Scoring Framework (Credit Card / Customer Fraud)\n",
        "\n",
        "## What this does\n",
        "This project builds a production-style fraud decisioning framework:\n",
        "- Train a supervised model to estimate fraud probability for transactions\n",
        "- Convert probability into a **0–1000 risk score**\n",
        "- Apply thresholds to produce decisions: **APPROVE / REVIEW / DECLINE**\n",
        "\n",
        "## Dataset\n",
        "Synthetic but realistic credit card transactions including:\n",
        "- Amount, merchant category, country\n",
        "- Card present vs not present\n",
        "- Velocity (1h/24h), device change\n",
        "- Failed logins (ATO proxy), travel flag\n",
        "- Customer tenure and prior chargebacks\n",
        "\n",
        "## Model\n",
        "- Logistic Regression (interpretable baseline)\n",
        "- Class imbalance handled via `class_weight=\"balanced\"`\n",
        "\n",
        "## Test Performance (probability model)\n",
        "- ROC-AUC: **{roc:.4f}**\n",
        "- Average Precision (PR-AUC): **{ap:.4f}**\n",
        "\n",
        "## Decisioning Logic (risk score thresholds)\n",
        "- 0–300: APPROVE\n",
        "- 301–700: REVIEW\n",
        "- 701–1000: DECLINE\n",
        "\n",
        "## Precision-based decline calibration (optional)\n",
        "A probability threshold targeting precision ~0.80 produced:\n",
        "- Threshold: **{decline_thr:.4f}**\n",
        "- Achieved precision: **{achieved_prec:.4f}**\n",
        "- Achieved recall: **{achieved_rec:.4f}**\n",
        "\n",
        "## Outputs\n",
        "- `outputs/scored_test_transactions.csv`\n",
        "- `outputs/decision_summary.csv`\n",
        "- `outputs/fraud_rate_by_decision.csv`\n",
        "- Charts: ROC, PR, confusion matrix, risk score distribution\n",
        "\"\"\"\n",
        "    with open(os.path.join(OUT_DIR, \"executive_summary.md\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(exec_md)\n",
        "\n",
        "    # Print quick console summary\n",
        "    print(\"✅ Done.\")\n",
        "    print(f\"ROOT: {ROOT}\")\n",
        "    print(f\"Data saved: {data_path}\")\n",
        "    print(f\"Scored output saved: {scored_path}\")\n",
        "    print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {ap:.4f}\\n\")\n",
        "    print(\"Decisioning evaluation (DECLINE as positive prediction):\")\n",
        "    print(report)\n",
        "    print(\"Saved outputs in:\", OUT_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qMg8vwR8NjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}